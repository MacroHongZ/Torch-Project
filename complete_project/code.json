{
    "main": "from data import load_dataset\nfrom model import MlpTrainer\nfrom deeplog import DeepLog, Config\nimport argparse\n\n\ndef load_config(model_name):\n    # =================== Model-1 config ===================\n    mlp_cf = Config()\n\n    # training config\n    mlp_cf.epoch = 100\n    mlp_cf.lr = 0.001\n    mlp_cf.batch_size = 400\n    mlp_cf.use_GPU = True\n    mlp_cf.device = \"cuda:0\"\n\n    mlp_cf.use_check_point = True\n    mlp_cf.ck_name = \"best_model.ckpt\"\n    mlp_cf.save_log = True\n\n    # model config\n    mlp_cf.in_dim = 64\n\n    # =================== Model-2 config ===================\n    model2_cf = Config()\n\n    # training config\n\n    # model config\n\n    if model_name == \"mlp\":\n        config = mlp_cf\n\n    return config\n\n\ndef prepare_data(dataset_name):\n    if dataset_name == \"data1\":\n        train_dataset, validate_dataset, test_dataset = load_dataset()\n\n    print(\"{:-^40s}\".format(\"Dataset info\"))\n    print(\"Train_dataset: \", len(train_dataset))\n    print(\"Validate_dataset: \", len(validate_dataset))\n    print(\"Test_dataset: \", len(test_dataset))\n    print(\"All dataset: \", len(train_dataset) + len(validate_dataset) + len(test_dataset))\n    print(\"-\" * 40)\n\n    return train_dataset, validate_dataset, test_dataset\n\n\ndef train_model(model_name, dataset, config):\n    if model_name == \"mlp\":\n        trainer = MlpTrainer(dataset, config)\n\n    trainer.train()\n    trainer.test()\n\n    if config.save_log:\n        trainer.save_log()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--lr', type=float, default=0.001)\n    parser.add_argument('--epoch', type=int, default=100)\n    parser.add_argument('--batch_size', type=int, default=400)\n    parser.add_argument('--device', default=\"cuda:0\")\n    parser.add_argument('--opt', default='Adam')\n    parser.add_argument('--lr_scheduler', action=\"store_true\")\n\n    parser.add_argument('--dataset_name', default='data1')\n    parser.add_argument('--model_name', default='mlp')\n\n    args = parser.parse_args()\n\n    dataset_name = args.dataset_name\n    model_name = args.model_name\n\n    dataset = prepare_data(dataset_name)\n\n    config = load_config(model_name)\n    config.from_argparse(args)  # This will overwrite the value of the same key parameter in config\n    config.print_parameters()\n\n    train_model(model_name, dataset, config)\n",
    "model": "import torch\nimport torch.nn as nn\n\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super(MLP, self).__init__()\n\n        in_dim = config.in_dim\n        self.mlp = nn.Sequential(\n            nn.Linear(in_dim, in_dim // 2),\n            nn.ReLU(),\n            nn.Linear(in_dim // 2, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.mlp(x)\n",
    "modelTra": "from deeplog import DeepLog, Config\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch import optim\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom sklearn import metrics\nimport numpy as np\n\nfrom .mlp import MLP\n\n\nclass MlpTrainer():\n    def __init__(self, data, config):\n\n        self.logger = DeepLog(save_path=\"output_files\")\n        self.config = config\n\n        self.train_dataloader = DataLoader(data[0], batch_size=config.batch_size, shuffle=True)\n        self.validate_dataloader = DataLoader(data[1], batch_size=config.batch_size, shuffle=True)\n        self.test_dataloader = DataLoader(data[2], batch_size=config.batch_size, shuffle=True)\n        self.model = MLP(config)\n        self.loss_fn = nn.BCELoss()\n        self.optimizer = optim.Adam(self.model.parameters(), lr=config.lr)\n        # self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=config.epoch)\n\n    def train(self):\n        print(\"Begin training........\")\n        total = sum([param.numel() for param in self.model.parameters()])\n        print(\"Number of parameter: \", total)\n\n        epoch_bar = tqdm(range(self.config.epoch))\n        for i in epoch_bar:\n            epoch_bar.set_description(f\"Epoch: {i}\")\n\n            train_loss = self.train_loop()\n            validate_loss = self.validate()\n\n            if self.config.use_check_point:\n                if self.config.is_best_epoch:\n                    best_state_dict = self.model.state_dict()\n\n            epoch_bar.set_postfix({\"Train_loss: \": train_loss, \"Validate_loss: \": validate_loss})\n        epoch_bar.close()\n\n        if self.config.use_check_point:\n            torch.save(best_state_dict, \"output_files/check_point/\" + self.config.ck_name)\n\n        print(\"End training........\")\n\n        self.logger.visualization(\"train_loss\")\n        self.logger.visualization(\"ACC\")\n\n    def train_loop(self):\n        if self.config.use_GPU:\n            device = torch.device(self.config.device)\n            self.model.to(device)\n\n        self.model.train()\n        for batch in self.train_dataloader:\n            x, y = batch\n            y = torch.unsqueeze(y, 1)\n\n            if self.config.use_GPU:\n                x = x.to(device)\n                y = y.to(device)\n\n            y_hat = self.model(x)\n            loss = self.loss_fn(y_hat, y)\n\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n            loss_value = loss.item()\n\n        self.logger.log(\"train_loss\", loss_value)\n\n        return loss_value\n\n    def validate(self):\n        if self.config.use_GPU:\n            device = torch.device(self.config.device)\n            self.model.to(device)\n\n        self.model.eval()\n        predictions = []\n        labels = []\n        with torch.no_grad():\n            for batch in self.validate_dataloader:\n                x, y = batch\n                y = torch.unsqueeze(y, 1)\n\n                if self.config.use_GPU:\n                    x = x.to(device)\n                    y = y.to(device)\n\n                y_hat = self.model(x)\n                loss = self.loss_fn(y_hat, y)\n                loss_value = loss.item()\n\n                labels.append(y)\n                predictions.append(y_hat)\n        self.logger.log(\"validate_loss\", loss_value)\n\n        predictions = torch.cat(predictions).cpu().numpy()\n        labels = torch.cat(labels).cpu().numpy()\n        AUC, AUPR, ACC, F1_score, Precision, Recall = get_metric(predictions, labels)\n\n        self.logger.log(\"AUC\", AUC)\n        self.logger.log(\"AUPR\", AUPR)\n        self.logger.log(\"ACC\", ACC)\n        self.logger.log(\"F1_score\", F1_score)\n        self.logger.log(\"Precision\", Precision)\n        self.logger.log(\"Recall\", Recall)\n\n        if self.config.use_check_point:\n            max_f1 = max(self.logger.logs[\"F1_score\"])\n            if F1_score >= max_f1:\n                self.config.is_best_epoch = True\n\n        return loss_value\n\n    def test(self):\n        print(\"Begin test........\")\n\n        if self.config.use_check_point:\n            best_state_dict = torch.load(\"output_files/check_point/best_model.ckpt\")\n            self.model.load_state_dict(best_state_dict)\n\n        if self.config.use_GPU:\n            device = torch.device(self.config.device)\n            self.model.to(device)\n\n        self.model.eval()\n        predictions = []\n        labels = []\n        with torch.no_grad():\n            for batch in self.test_dataloader:\n                x, y = batch\n                y = torch.unsqueeze(y, 1)\n\n                if self.config.use_GPU:\n                    x = x.to(device)\n\n                y_hat = self.model(x)\n\n                labels.append(y)\n                predictions.append(y_hat)\n\n        predictions = torch.cat(predictions).cpu().numpy()\n        labels = torch.cat(labels).cpu().numpy()\n        AUC, AUPR, ACC, F1_score, Precision, Recall = get_metric(predictions, labels)\n\n        test_result = \"AUC: {:.2f} \\nAUPR: {:.2f} \\nACC: {:.2f} \\nF1_score: {:.2f} \\nPrecision: {:.2f} \\nRecall: {:.2f}\"\n        test_result = test_result.format(AUC, AUPR, ACC, F1_score, Precision, Recall)\n\n        self.logger.log(\"test_result\", (AUC, AUPR, ACC, F1_score, Precision, Recall))\n\n        print(\"{:-^40s}\".format(\"Test_result\"))\n        print(test_result)\n        print(\"-\" * 40)\n        print(\"End test........\")\n\n    def save_log(self):\n        self.logger.save(config=self.config)\n\n\ndef get_metric(predictions, labels):\n    \"\"\"\n    :param predictions: np.array with shape (n,)\n    :param labels: np.array with shape (n,)\n    :return: all float\n    \"\"\"\n    fpr, tpr, thresholds = metrics.roc_curve(labels, predictions)\n    Precision, Recall, thresholds = metrics.precision_recall_curve(labels, predictions)\n    AUC = metrics.auc(fpr, tpr)\n    AUPR = metrics.auc(Recall, Precision)\n\n    F1_score = 2 * (Precision * Recall) / ((Precision + Recall) + 1e-15)\n    index = np.argmax(F1_score)\n    F1_score = F1_score[index]\n    Precision = Precision[index]\n    Recall = Recall[index]\n\n    threshold = thresholds[index]\n\n    predictions[predictions > threshold] = 1\n    predictions[predictions <= threshold] = 0\n\n    ACC = metrics.accuracy_score(labels, predictions)\n\n    return AUC, AUPR, ACC, F1_score, Precision, Recall\n",
    "data": "import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_classification\n\n\nclass MyDataset(Dataset):\n    def __init__(self, data, label):\n        super(MyDataset, self).__init__()\n\n        self.data = data\n        self.label = label\n\n    def __getitem__(self, item):\n        data = self.data[item]\n        label = self.label[item]\n\n        return data, label\n\n    def __len__(self):\n        return self.data.shape[0]\n\n\ndef load_dataset():\n    # load your dataset here\n    data, label = make_classification(n_samples=6000, n_features=64, n_informative=32, n_clusters_per_class=1)\n    data = torch.from_numpy(data).float()\n    label = torch.from_numpy(label).float()\n    data_size = data.shape[0]\n\n    spilt_index = data_split(data_size, [0.8, 0.1, 0.1])\n    train_index, validate_index, test_index = spilt_index\n\n    train_dataset = MyDataset(data[train_index], label[train_index])\n    validate_dataset = MyDataset(data[validate_index], label[validate_index])\n    test_dataset = MyDataset(data[test_index], label[test_index])\n\n    return train_dataset, validate_dataset, test_dataset\n\n\ndef data_split(data_size, ratio, itype=\"array\", seed=42):\n    np.random.seed(seed)\n    ratio = np.array(ratio)\n    numbers = (ratio * data_size).astype(np.int32)\n    cu_numbers = np.zeros(numbers.size + 1, dtype=np.int32)\n    cu_numbers[1:] = np.cumsum(numbers)\n\n    index = np.arange(data_size)\n    np.random.shuffle(index)\n    if itype == \"tensor\":\n        index = torch.from_numpy(index)\n\n    split_index = []\n    for i in range(numbers.size):\n        if i == (numbers.size - 1):\n            split_index.append(index[cu_numbers[i]:])\n        else:\n            split_index.append(index[cu_numbers[i]:cu_numbers[i + 1]])\n\n    return split_index\n"
}