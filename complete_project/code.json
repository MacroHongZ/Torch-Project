{
    "main": "\nfrom data import DemoDataset\nfrom model import MlpTrainer\n\ntrain_dataset = DemoDataset(type=\"train\")\nvalidate_dataset = DemoDataset(type=\"validate\")\ntest_dataset = DemoDataset(type=\"test\")\nprint(\"=\" * 25)\nprint(\"train_dataset:\", len(train_dataset), \"data:\", train_dataset.data.shape, \"label:\", train_dataset.label.shape,\n      sep=\" \")\nprint(\"validate_dataset: \", len(validate_dataset), \"data:\", validate_dataset.data.shape, \"label:\", validate_dataset.label.shape,\n      sep=\" \")\nprint(\"test_dataset: \", len(test_dataset), \"data:\", test_dataset.data.shape, \"label:\", test_dataset.label.shape,\n      sep=\" \")\nprint(\"=\" * 25)\n\ntrainer = MlpTrainer((train_dataset, validate_dataset, test_dataset))\ntrainer.train()\ntrainer.test()\ntrainer.save_log()\n",
    "model": "import torch\nimport torch.nn as nn\n\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super(MLP, self).__init__()\n\n        in_dim = config.in_dim\n        self.mlp = nn.Sequential(\n            nn.Linear(in_dim, in_dim // 2),\n            nn.ReLU(),\n            nn.Linear(in_dim // 2, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.mlp(x)\n",
    "modelTra": "from deeplog import DeepLog, Config\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch import optim\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom sklearn import metrics\nimport numpy as np\n\nfrom .mlp import MLP\n\n\nclass MlpTrainer():\n    def __init__(self, data):\n        config = Config()\n        self.log = DeepLog(save_path=\"output_files\")\n\n        # training config\n        config.epoch = 100\n        config.lr = 0.001\n        config.batch_size = 200\n        config.device = \"cpu\"\n\n        # model config\n        config.in_dim = 64\n\n        self.config = config\n        self.train_dataloader = DataLoader(data[0], batch_size=config.batch_size, shuffle=True)\n        self.validate_dataloader = DataLoader(data[1], batch_size=config.batch_size, shuffle=True)\n        self.test_dataloader = DataLoader(data[2], batch_size=config.batch_size, shuffle=True)\n        self.model = MLP(config)\n        self.loss_fn = nn.BCELoss()\n        self.optimizer = optim.Adam(self.model.parameters(), lr=config.lr)\n        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=config.epoch)\n\n    def train(self, use_check_point=False):\n        print(\"Begin training........\")\n        device = torch.device(self.config.device)\n        self.model.to(device)\n\n        pbar = tqdm(range(self.config.epoch))\n        max_ACC = 0\n        for i in pbar:\n            pbar.set_description(f\"Epoch: {i}\")\n            self.model.train()\n            for batch in self.train_dataloader:\n                x, y = batch\n                x = x.to(device)\n                y = y.to(device)\n                y = torch.unsqueeze(y, 1)\n\n                y_hat = self.model(x)\n                loss = self.loss_fn(y_hat, y)\n\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n\n            self.log.log(\"loss\", loss.item())\n\n            self.model.eval()\n            predictions = []\n            labels = []\n            with torch.no_grad():\n                for batch in self.validate_dataloader:\n                    x, y = batch\n                    x = x.to(device)\n                    labels.append(y)\n                    y_hat = self.model(x)\n                    predictions.append(y_hat)\n                predictions = torch.cat(predictions).cpu().numpy()\n                labels = torch.cat(labels).cpu().numpy()\n                AUC, AUPR, ACC, F1_score, precision, recall = get_metric(predictions, labels)\n                if ACC > max_ACC:\n                    max_ACC = ACC\n                    if use_check_point:\n                        best_state_dict = self.model.state_dict()\n\n                pbar.set_postfix({\"loss\": str(loss.item())[:6], \"acc\": ACC})\n                self.log.log(\"val_ACC\", ACC)\n\n        if use_check_point:\n            torch.save(best_state_dict, \"output_files/check_point/best_model.ckpt\")\n        pbar.close()\n\n        self.log.visualization(item=\"loss\")\n        self.log.visualization(item=\"val_ACC\")\n\n    def test(self, use_check_point=False):\n        print(\"Begin test........\")\n        device = torch.device(self.config.device)\n        self.model.to(device)\n\n        if use_check_point:\n            best_state_dict = torch.load(\"output_files/check_point/best_model.ckpt\")\n            self.model.load_state_dict(best_state_dict)\n\n        self.model.eval()\n        predictions = []\n        labels = []\n        with torch.no_grad():\n            for batch in self.test_dataloader:\n                x, y = batch\n                x = x.to(device)\n                labels.append(y)\n                y_hat = self.model(x)\n                predictions.append(y_hat)\n            predictions = torch.cat(predictions).cpu().numpy()\n            labels = torch.cat(labels).cpu().numpy()\n            AUC, AUPR, ACC, F1_score, precision, recall = get_metric(predictions, labels)\n            self.log.log(\"test_ACC\", ACC)\n            print(\"=\" * 25)\n            print(\"test_ACC: \", ACC)\n            print(\"=\" * 25)\n\n    def save_log(self):\n        self.log.save()\n\n\ndef get_metric(predictions, labels):\n    \"\"\"\n    :param predictions: np.array with shape (n,)\n    :param labels: np.array with shape (n,)\n    :return: all float\n    \"\"\"\n    fpr, tpr, thresholds = metrics.roc_curve(labels, predictions)\n    precision, recall, thresholds = metrics.precision_recall_curve(labels, predictions)\n    AUC = metrics.auc(fpr, tpr)\n    AUPR = metrics.auc(recall, precision)\n\n    F1_score = 2 * (precision * recall) / ((precision + recall) + 1e-15)\n    index = np.argmax(F1_score)\n    F1_score = F1_score[index]\n    precision = precision[index]\n    recall = recall[index]\n\n    threshold = thresholds[index]\n\n    predictions[predictions > threshold] = 1\n    predictions[predictions <= threshold] = 0\n\n    ACC = metrics.accuracy_score(labels, predictions)\n\n    return AUC, AUPR, ACC, F1_score, precision, recall\n",
    "data": "import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_classification\n\n\nclass DemoDataset(Dataset):\n    def __init__(self, type=\"train\"):\n        super(DemoDataset, self).__init__()\n\n        data, label = make_classification(n_samples=6000, n_features=64, n_informative=32, n_clusters_per_class=1)\n        data_size = data.shape[0]\n\n        spilt_index = data_split(data_size, [0.8, 0.1, 0.1])\n        if type == \"train\":\n            data = data[spilt_index[0]]\n            label = label[spilt_index[0]]\n        elif type == \"validate\":\n            data = data[spilt_index[1]]\n            label = label[spilt_index[1]]\n        else:\n            data = data[spilt_index[2]]\n            label = label[spilt_index[2]]\n\n        self.data = torch.tensor(data).float()\n        self.label = torch.tensor(label).float()\n\n    def __getitem__(self, item):\n        data = self.data[item]\n        label = self.label[item]\n\n        return data, label\n\n    def __len__(self):\n        return self.data.shape[0]\n\n\ndef data_split(data_size, ratio):\n    ratio = np.array(ratio)\n    numbers = (ratio * data_size).astype(np.int32)\n    cu_numbers = np.zeros(numbers.size + 1, dtype=np.int32)\n    cu_numbers[1:] = np.cumsum(numbers)\n\n    index = np.arange(data_size)\n    np.random.shuffle(index)\n\n    split_index = []\n    for i in range(numbers.size):\n        if i == (numbers.size - 1):\n            split_index.append(index[cu_numbers[i]:])\n        else:\n            split_index.append(index[cu_numbers[i]:cu_numbers[i + 1]])\n\n    return split_index\n"
}