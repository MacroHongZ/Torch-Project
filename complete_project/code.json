{
    "main": "from data import load_dataset\nfrom model import MlpTrainer\n\ntrain_dataset, validate_dataset, test_dataset = load_dataset()\nprint(\"{:-^40s}\".format(\"Dataset info\"))\nprint(\"Train_dataset: \", len(train_dataset))\nprint(\"Validate_dataset: \", len(validate_dataset))\nprint(\"Test_dataset: \", len(test_dataset))\nprint(\"All dataset: \", len(train_dataset) + len(validate_dataset) + len(test_dataset))\nprint(\"-\" * 40)\n\ntrainer = MlpTrainer((train_dataset, validate_dataset, test_dataset))\ntrainer.train()\ntrainer.test()\ntrainer.save_log()\n",
    "model": "import torch\nimport torch.nn as nn\n\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super(MLP, self).__init__()\n\n        in_dim = config.in_dim\n        self.mlp = nn.Sequential(\n            nn.Linear(in_dim, in_dim // 2),\n            nn.ReLU(),\n            nn.Linear(in_dim // 2, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.mlp(x)\n",
    "modelTra": "from deeplog import DeepLog, Config\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch import optim\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom sklearn import metrics\nimport numpy as np\n\nfrom .mlp import MLP\n\n\nclass MlpTrainer():\n    def __init__(self, data):\n        config = Config()\n        self.logger = DeepLog(save_path=\"output_files\")\n\n        # training config\n        config.epoch = 100\n        config.lr = 0.001\n        config.batch_size = 400\n        config.use_GPU = True\n        config.device = \"cuda:0\"\n\n        # model config\n        config.in_dim = 64\n\n        self.config = config\n        self.train_dataloader = DataLoader(data[0], batch_size=config.batch_size, shuffle=True)\n        self.validate_dataloader = DataLoader(data[1], batch_size=config.batch_size, shuffle=True)\n        self.test_dataloader = DataLoader(data[2], batch_size=config.batch_size, shuffle=True)\n        self.model = MLP(config)\n        self.loss_fn = nn.BCELoss()\n        self.optimizer = optim.Adam(self.model.parameters(), lr=config.lr)\n        # self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=config.epoch)\n\n    def train(self, use_check_point=False):\n        print(\"Begin training........\")\n        total = sum([param.numel() for param in self.model.parameters()])\n        print(\"Number of parameter: \", total)\n\n        epoch_bar = tqdm(range(self.config.epoch))\n        for i in epoch_bar:\n            epoch_bar.set_description(f\"Epoch: {i}\")\n\n            train_loss = self.train_loop()\n            validate_loss = self.validate(use_check_point)\n\n            epoch_bar.set_postfix({\"Train_loss: \": train_loss, \"Validate_loss: \": validate_loss})\n        epoch_bar.close()\n        print(\"End training........\")\n\n        self.logger.visualization(\"train_loss\")\n        self.logger.visualization(\"ACC\")\n\n    def train_loop(self):\n        if self.config.use_GPU:\n            device = torch.device(self.config.device)\n            self.model.to(device)\n\n        self.model.train()\n        for batch in self.train_dataloader:\n            x, y = batch\n            y = torch.unsqueeze(y, 1)\n\n            if self.config.use_GPU:\n                x = x.to(device)\n                y = y.to(device)\n\n            y_hat = self.model(x)\n            loss = self.loss_fn(y_hat, y)\n\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n            loss_value = loss.item()\n\n            self.logger.log(\"train_loss\", loss_value)\n\n        return loss_value\n\n    def validate(self, use_check_point):\n        if self.config.use_GPU:\n            device = torch.device(self.config.device)\n            self.model.to(device)\n\n        self.model.eval()\n        predictions = []\n        labels = []\n        with torch.no_grad():\n            for batch in self.validate_dataloader:\n                x, y = batch\n                y = torch.unsqueeze(y, 1)\n\n                if self.config.use_GPU:\n                    x = x.to(device)\n                    y = y.to(device)\n\n                y_hat = self.model(x)\n                loss = self.loss_fn(y_hat, y)\n                loss_value = loss.item()\n\n                labels.append(y)\n                predictions.append(y_hat)\n                self.logger.log(\"validate_loss\", loss_value)\n\n        predictions = torch.cat(predictions).cpu().numpy()\n        labels = torch.cat(labels).cpu().numpy()\n        AUC, AUPR, ACC, F1_score, Precision, Recall = get_metric(predictions, labels)\n\n        self.logger.log(\"AUC\", AUC)\n        self.logger.log(\"AUPR\", AUPR)\n        self.logger.log(\"ACC\", ACC)\n        self.logger.log(\"F1_score\", F1_score)\n        self.logger.log(\"Precision\", Precision)\n        self.logger.log(\"Recall\", Recall)\n\n        if use_check_point:\n            max_f1 = max(self.logger.logs[\"F1_score\"])\n            if F1_score >= max_f1:\n                best_state_dict = self.model.state_dict()\n                torch.save(best_state_dict, \"output_files/check_point/best_model.ckpt\")\n\n        return loss_value\n\n    def test(self, use_check_point=False):\n        print(\"Begin test........\")\n\n        if use_check_point:\n            best_state_dict = torch.load(\"output_files/check_point/best_model.ckpt\")\n            self.model.load_state_dict(best_state_dict)\n\n        if self.config.use_GPU:\n            device = torch.device(self.config.device)\n            self.model.to(device)\n\n        self.model.eval()\n        predictions = []\n        labels = []\n        with torch.no_grad():\n            for batch in self.test_dataloader:\n                x, y = batch\n                y = torch.unsqueeze(y, 1)\n\n                if self.config.use_GPU:\n                    x = x.to(device)\n\n                y_hat = self.model(x)\n\n                labels.append(y)\n                predictions.append(y_hat)\n\n        predictions = torch.cat(predictions).cpu().numpy()\n        labels = torch.cat(labels).cpu().numpy()\n        AUC, AUPR, ACC, F1_score, Precision, Recall = get_metric(predictions, labels)\n\n        test_result = \"AUC: {:.2f} \\nAUPR: {:.2f} \\nACC: {:.2f} \\nF1_score: {:.2f} \\nPrecision: {:.2f} \\nRecall: {:.2f}\"\n        test_result = test_result.format(AUC, AUPR, ACC, F1_score, Precision, Recall)\n\n        self.logger.log(\"test_result\", test_result)\n\n        print(\"{:-^40s}\".format(\"Test_result\"))\n        print(test_result)\n        print(\"=\" * 25)\n        print(\"End test........\")\n\n    def save_log(self):\n        self.logger.save()\n\n\ndef get_metric(predictions, labels):\n    \"\"\"\n    :param predictions: np.array with shape (n,)\n    :param labels: np.array with shape (n,)\n    :return: all float\n    \"\"\"\n    fpr, tpr, thresholds = metrics.roc_curve(labels, predictions)\n    Precision, Recall, thresholds = metrics.precision_recall_curve(labels, predictions)\n    AUC = metrics.auc(fpr, tpr)\n    AUPR = metrics.auc(Recall, Precision)\n\n    F1_score = 2 * (Precision * Recall) / ((Precision + Recall) + 1e-15)\n    index = np.argmax(F1_score)\n    F1_score = F1_score[index]\n    Precision = Precision[index]\n    Recall = Recall[index]\n\n    threshold = thresholds[index]\n\n    predictions[predictions > threshold] = 1\n    predictions[predictions <= threshold] = 0\n\n    ACC = metrics.accuracy_score(labels, predictions)\n\n    return AUC, AUPR, ACC, F1_score, Precision, Recall\n",
    "data": "import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_classification\n\n\nclass MyDataset(Dataset):\n    def __init__(self, data, label):\n        super(MyDataset, self).__init__()\n\n        self.data = data\n        self.label = label\n\n    def __getitem__(self, item):\n        data = self.data[item]\n        label = self.label[item]\n\n        return data, label\n\n    def __len__(self):\n        return self.data.shape[0]\n\n\ndef load_dataset():\n    # load your dataset here\n    data, label = make_classification(n_samples=6000, n_features=64, n_informative=32, n_clusters_per_class=1)\n    data = torch.from_numpy(data).float()\n    label = torch.from_numpy(label).float()\n    data_size = data.shape[0]\n\n    spilt_index = data_split(data_size, [0.8, 0.1, 0.1])\n    train_index, validate_index, test_index = spilt_index\n\n    train_dataset = MyDataset(data[train_index], label[train_index])\n    validate_dataset = MyDataset(data[validate_index], label[validate_index])\n    test_dataset = MyDataset(data[test_index], label[test_index])\n\n    return train_dataset, validate_dataset, test_dataset\n\n\ndef data_split(data_size, ratio, itype=\"array\"):\n    ratio = np.array(ratio)\n    numbers = (ratio * data_size).astype(np.int32)\n    cu_numbers = np.zeros(numbers.size + 1, dtype=np.int32)\n    cu_numbers[1:] = np.cumsum(numbers)\n\n    index = np.arange(data_size)\n    np.random.shuffle(index)\n    if itype == \"tensor\":\n        index = torch.from_numpy(index)\n\n    split_index = []\n    for i in range(numbers.size):\n        if i == (numbers.size - 1):\n            split_index.append(index[cu_numbers[i]:])\n        else:\n            split_index.append(index[cu_numbers[i]:cu_numbers[i + 1]])\n\n    return split_index\n"
}