{
    "main": "import torch\nfrom torch.utils.data import DataLoader\nfrom deeplog import DeepLog, Config\nfrom torch import optim\nimport torch.nn as nn\nfrom tqdm import tqdm\n\nfrom model.model import Model\nfrom data.data import MyDataset\nfrom utils import get_metric\n\n\ndef train(model, optimizer, loss_fn, train_dataloader, validate_dataloader, test_dataloader, metric, config, log,\n          use_check_point=False):\n    print(\"Begin training........\")\n    device = torch.device(config.device)\n    model.to(device)\n\n    pbar = tqdm(range(config.epoch))\n    max_ACC = 0\n    for i in pbar:\n        pbar.set_description(f\"Epoch: {i}\")\n        model.train()\n        for batch in train_dataloader:\n            x, y = batch\n            x = x.to(device)\n            y = y.to(device)\n\n            y_hat = model(x)\n            loss = loss_fn(y_hat, y)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            log.log(\"loss\", loss.item())\n        pbar.set_postfix({\"loss\": str(loss.item())})\n\n        model.eval()\n        predictions = []\n        labels = []\n        with torch.no_grad():\n            for batch in validate_dataloader:\n                x, y = batch\n                x = x.to(device)\n                labels.append(y)\n                y_hat = model(x)\n                predictions.append(y_hat)\n            predictions = torch.cat(predictions).cpu().numpy()\n            labels = torch.cat(labels).cpu().numpy()\n            ACC = metric(predictions, labels)\n            if ACC > max_ACC:\n                max_ACC = ACC\n                if use_check_point:\n                    best_state_dict = model.state_dict()\n\n            print(\"ACC: \", ACC)\n            log.log(\"ACC\", ACC)\n\n    if use_check_point:\n        torch.save(best_state_dict, \"output_file/check_point/best_model.ckpt\")\n    pbar.close()\n\n\ndef test(model, test_dataloader, metric, config, log, use_check_point=False):\n    print(\"Begin test........\")\n    device = torch.device(config.device)\n    model.to(device)\n\n    if use_check_point:\n        best_state_dict = torch.load(\"output_file/check_point/best_model.ckpt\")\n        model.load_state_dict(best_state_dict)\n\n    model.eval()\n    predictions = []\n    labels = []\n    with torch.no_grad():\n        for batch in test_dataloader:\n            x, y = batch\n            x = x.to(device)\n            labels.append(y)\n            y_hat = model(x)\n            predictions.append(y_hat)\n        predictions = torch.cat(predictions).cpu().numpy()\n        labels = torch.cat(labels).cpu().numpy()\n        ACC = metric(predictions, labels)\n        log.log(\"test ACC\", ACC)\n        print(\"=\" * 25)\n        print(\"ACC: \", ACC)\n        print(\"=\" * 25)\n\n\ndef main():\n    config = Config()\n    log = DeepLog(save_path=\"output_files\")\n\n    config.epoch = 200\n    config.lr = 0.001\n    config.batch_size = 100\n    config.in_dim = 512\n    config.device = \"cuda:0\"\n\n    config.print_parameters()\n\n    train_dataset = MyDataset(type=\"train\")\n    validate_dataset = MyDataset(type=\"validate\")\n    test_dataset = MyDataset(type=\"test\")\n    print(\"=\" * 25)\n    print(\"train_dataset: \", len(train_dataset))\n    print(\"validate_dataset: \", len(validate_dataset))\n    print(\"test_dataset: \", len(test_dataset))\n    print(\"=\" * 25)\n\n    train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n    validate_dataloader = DataLoader(validate_dataset, batch_size=config.batch_size, shuffle=True)\n    test_dataloader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=True)\n\n    model = Model(config)\n    loss_fn = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epoch)\n\n    train(model=model,\n          optimizer=optimizer,\n          loss_fn=loss_fn,\n          train_dataloader=train_dataloader,\n          validate_dataloader=validate_dataloader,\n          test_dataloader=test_dataloader,\n          metric=get_metric,\n          config=config,\n          log=log)\n\n    test(model=model,\n         test_dataloader=test_dataloader,\n         metric=get_metric,\n         config=config,\n         log=log)\n\n    log.visualization(item=\"loss\")\n    log.save(config=config, config_save=True)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "utils": "\ndef get_metric(predictions, labels):\n    \"\"\"\n    :param predictions: np.array with shape (n,)\n    :param labels: np.array with shape (n,)\n    :return: all float\n    \"\"\"\n    pass\n",
    "model": "import torch\nimport torch.nn as nn\n\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n\n\n    def forward(self):\n\n\n\n",
    "data": "import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport pandas as pd\n\n\nclass MyDataset(Dataset):\n    def __init__(self, type=\"train\"):\n        super(MyDataset, self).__init__()\n\n        data = np.load(\"file_path\")\n        label = np.load(\"file_path\")\n        data_size = data.shape[0]\n\n        spilt_index = data_split(data_size, [0.8, 0.1, 0.1])\n        if type == \"train\":\n            data = data[spilt_index[0]]\n        elif type == \"validate\":\n            data = data[spilt_index[1]]\n        else:\n            data = data[spilt_index[2]]\n\n        self.data = torch.tensor(data)\n        self.label = label\n\n    def __getitem__(self, item):\n        data = self.data[item]\n        label = self.label[item]\n\n        return data, label\n\n    def __len__(self):\n        return self.data.shape[0]\n\n\ndef data_split(data_size, ratio):\n    ratio = np.array(ratio)\n    numbers = (ratio * data_size).astype(np.int32)\n    cu_numbers = np.zeros(numbers.size + 1, dtype=np.int32)\n    cu_numbers[1:] = np.cumsum(numbers)\n\n    index = np.arange(data_size)\n    np.random.shuffle(index)\n\n    split_index = []\n    for i in range(numbers.size):\n        if i == (numbers.size - 1):\n            split_index.append(index[cu_numbers[i]:])\n        else:\n            split_index.append(index[cu_numbers[i]:cu_numbers[i + 1]])\n\n    return split_index\n"
}