{
    "main": "import torch\nfrom torch.utils.data import DataLoader\nfrom deeplog import DeepLog, Config\nfrom torch import optim\nimport torch.nn as nn\nfrom tqdm import tqdm\n\nfrom model import MLP\nfrom data import load_dataset\nfrom utils import get_metric\n\n\nclass Trainer:\n    def __init__(\n            self,\n            model,\n            dataset,\n            lr,\n            epoch,\n            batch_size,\n            use_GPU,\n            device,\n            save_checkpoint,\n            checkpoint_name,\n    ):\n        self.logger = DeepLog()\n\n        self.train_data = dataset[0]\n        self.validate_data = dataset[1]\n        self.test_data = dataset[2]\n\n        self.model = model\n        self.loss_fn = nn.BCELoss()\n        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n        self.metric_fn = get_metric\n\n        self.max_metric = -float(\"inf\")\n        self.lr = lr\n        self.epoch = epoch\n        self.batch_size = batch_size\n        self.use_GPU = use_GPU\n        self.device = device\n        self.save_checkpoint = save_checkpoint\n        self.checkpoint_name = checkpoint_name\n\n        if use_GPU:\n            self.model.to(self.device)\n\n    def train(self):\n        print(\"Begin training........\")\n\n        epoch_bar = tqdm(range(self.epoch))\n        for i in epoch_bar:\n            epoch_bar.set_description(f\"Epoch: {i}\")\n\n            self.model.train()\n            loss = None\n            train_dataloader = DataLoader(\n                self.train_data, batch_size=self.batch_size, shuffle=True\n            )\n            for batch in train_dataloader:\n                x, y = batch\n                y = torch.unsqueeze(y, 1)\n\n                if self.use_GPU:\n                    x = x.to(self.device)\n                    y = y.to(self.device)\n\n                self.optimizer.zero_grad()\n                y_hat = self.model(x)\n                loss = self.loss_fn(y_hat, y)\n                loss.backward()\n                self.optimizer.step()\n\n            loss_value = loss.item()\n            print(f\"Train loss: {loss_value:.4f}\")\n            self.logger.log(\"train_loss\", loss_value)\n\n            if i % 3 == 0:\n                self.validate()\n        epoch_bar.close()\n\n        self.logger.visualization(\"train_loss\")\n        self.logger.visualization(\"validate_loss\")\n\n    def validate(self):\n        self.model.eval()\n        loss_values = []\n        predictions = []\n        labels = []\n        validation_dataloader = DataLoader(\n            self.validate_data, batch_size=self.batch_size, shuffle=True\n        )\n        with torch.no_grad():\n            for batch in validation_dataloader:\n                x, y = batch\n                y = torch.unsqueeze(y, 1)\n\n                if self.use_GPU:\n                    x = x.to(self.device)\n                    y = y.to(self.device)\n\n                y_hat = self.model(x)\n                loss = self.loss_fn(y_hat, y)\n                loss_values.append(loss.detach().item())\n                labels.append(y)\n                predictions.append(y_hat)\n        predictions = torch.cat(predictions).cpu().numpy()\n        labels = torch.cat(labels).cpu().numpy()\n        mean_loss = torch.tensor(loss_values).mean().item()\n        print(f\"Validate loss: {mean_loss:.4f}\")\n        self.logger.log(\"validate_loss\", mean_loss)\n        metrics = get_metric(predictions, labels)\n\n        if self.save_checkpoint:\n            auc = metrics[\"AUC_ROC\"]\n            if auc > self.max_metric:\n                self.max_metric = auc\n                self.save_model()\n\n        for key, value in metrics.items():\n            self.logger.log(key, value)\n\n    def test(self):\n        print(\"Begin test........\")\n\n        if self.save_checkpoint:\n            state_dict = torch.load(\"output_files/check_point/\" + self.checkpoint_name)\n            self.model.load_state_dict(state_dict)\n\n        self.model.eval()\n        predictions = []\n        labels = []\n        test_dataloader = DataLoader(\n            self.test_data, batch_size=self.batch_size, shuffle=True\n        )\n        with torch.no_grad():\n            for batch in test_dataloader:\n                x, y = batch\n                y = torch.unsqueeze(y, 1)\n\n                if self.use_GPU:\n                    x = x.to(self.device)\n                    y = y.to(self.device)\n\n                y_hat = self.model(x)\n                labels.append(y)\n                predictions.append(y_hat)\n        predictions = torch.cat(predictions).cpu().numpy()\n        labels = torch.cat(labels).cpu().numpy()\n        metrics = get_metric(predictions, labels)\n\n        print(\"{:-^40s}\".format(\"Test result\"))\n        for key, value in metrics.items():\n            print(f\"{key:<10}: {value:.4f}\")\n        print(\"-\" * 40)\n\n        self.logger.log(\"test_result\", metrics)\n\n    def save_model(self):\n        torch.save(\n            self.model.state_dict(), \"output_files/check_point/\" + self.checkpoint_name\n        )\n\n    def save_log(self, config):\n        self.logger.save(config)\n\n\ndef main():\n    config = Config()\n    config.in_dim = 64\n\n    config.lr = 0.001\n    config.epoch = 50\n    config.batch_size = 256\n\n    config.use_GPU = True\n    config.device = \"cuda:0\"\n    config.save_checkpoint = True\n    config.checkpoint_name = \"best_model.pt\"\n\n    dataset = load_dataset()\n    model = MLP(in_dim=config.in_dim)\n\n    trainer = Trainer(\n        model=model,\n        dataset=dataset,\n        lr=config.lr,\n        epoch=config.epoch,\n        batch_size=config.batch_size,\n        use_GPU=config.use_GPU,\n        device=config.device,\n        save_checkpoint=config.save_checkpoint,\n        checkpoint_name=config.checkpoint_name,\n    )\n    trainer.train()\n    trainer.test()\n    trainer.save_log(config)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "utils": "from sklearn.metrics import (\n    auc,\n    accuracy_score,\n    precision_recall_curve,\n    roc_curve,\n)\n\nimport numpy as np\n\n\ndef get_metric(predictions: np.ndarray, labels: np.ndarray) -> dict:\n    \"\"\"\n    input::\n        - predictions: np.ndarray (n,)\n        - labels: np.ndarray (n,)\n\n    output::\n        - metrics_dict: dict of metrics\n    \"\"\"\n\n    fpr, tpr, roc_thresholds = roc_curve(labels, predictions)\n    precision, recall, pr_thresholds = precision_recall_curve(labels, predictions)\n    auc_roc = auc(fpr, tpr)\n    auc_pr = auc(recall, precision)\n\n    f1_scores = 2 * (precision * recall) / ((precision + recall) + 1e-15)\n    index = np.argmax(f1_scores)\n    f1_score = f1_scores[index]\n    precision = precision[index]\n    recall = recall[index]\n\n    threshold = pr_thresholds[index]\n    predictions[predictions > threshold] = 1\n    predictions[predictions <= threshold] = 0\n    accuracy = accuracy_score(labels, predictions)\n\n    metric_names = [\"AUC_ROC\", \"AUC_PR\", \"Accuracy\", \"F1_Score\", \"Precision\", \"Recall\"]\n    metric_values = [auc_roc, auc_pr, accuracy, f1_score, precision, recall]\n    metrics_dict = dict(zip(metric_names, metric_values))\n\n    return metrics_dict\n\n\ndef split_data(data_size: int, ratios: list, seed=42) -> list:\n    \"\"\"\n    input::\n        - data_size (int): The total size of the data.\n        - ratios (list): A list of ratios specifying the proportions of each subset.\n        - seed (int, optional): The random seed for reproducible shuffling. Defaults to 42.\n\n    output::\n        - list: A list of indices(np.ndarray) representing the split subsets.\n    \"\"\"\n\n    np.random.seed(seed)\n    n_subsets = len(ratios)\n\n    ratios = np.array(ratios)\n    split_sizes = (ratios * data_size).astype(np.int32)\n    cumulative_sizes = np.zeros(n_subsets + 1, dtype=np.int32)\n    cumulative_sizes[1:] = np.cumsum(split_sizes)\n    cumulative_sizes[-1] = data_size\n\n    indices = np.random.permutation(data_size)\n    split_indices = [\n        indices[cumulative_sizes[i] : cumulative_sizes[i + 1]] for i in range(n_subsets)\n    ]\n\n    return split_indices\n",
    "model": "import torch.nn as nn\n\n\nclass MLP(nn.Module):\n    def __init__(self, in_dim):\n        super().__init__()\n\n        self.mlp = nn.Sequential(\n            nn.Linear(in_dim, in_dim // 2),\n            nn.ReLU(),\n            nn.Linear(in_dim // 2, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        return self.mlp(x)\n",
    "data": "import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_classification\n\n\nfrom utils import split_data\n\n\nclass MyDataset(Dataset):\n    def __init__(self, data, label):\n        super(MyDataset, self).__init__()\n\n        self.data = data\n        self.label = label\n\n    def __getitem__(self, item):\n        data = self.data[item]\n        label = self.label[item]\n\n        return data, label\n\n    def __len__(self):\n        return self.data.shape[0]\n\n\ndef load_dataset():\n    print(\"Begin load dataset......\")\n    data, label = make_classification(\n        n_samples=6000, n_features=64, n_informative=32, n_clusters_per_class=1\n    )\n    data = torch.from_numpy(data).float()\n    label = torch.from_numpy(label).float()\n    data_size = data.shape[0]\n\n    spilt_index = split_data(data_size, [0.8, 0.1, 0.1])\n\n    datasets = []\n    for index in spilt_index:\n        index = torch.from_numpy(index).to(torch.long)\n        dataset = MyDataset(data[index], label[index])\n        datasets.append(dataset)\n\n    print(\"{:-^40s}\".format(\"Dataset info\"))\n    print(\"Train_dataset: \", len(datasets[0]))\n    print(\"Validate_dataset: \", len(datasets[1]))\n    print(\"Test_dataset: \", len(datasets[2]))\n    print(\"All dataset: \", len(datasets[0]) + len(datasets[1]) + len(datasets[2]))\n    print(\"-\" * 40)\n\n    return datasets\n"
}