{
    "main": "import torch\nfrom torch.utils.data import DataLoader\nfrom deeplog import DeepLog, Config\nfrom torch import optim\nimport torch.nn as nn\nfrom tqdm import tqdm\n\nfrom model.model import MLP\nfrom data.data import load_dataset\nfrom utils import get_metric\n\n\ndef train(model, optimizer, loss_fn, train_dataloader, validate_dataloader, test_dataloader, metric, config, logger,\n          use_check_point=False):\n    print(\"Begin training........\")\n\n    if config.use_GPU:\n        device = torch.device(config.device)\n        model.to(device)\n\n    epoch_bar = tqdm(range(config.epoch))\n    for i in epoch_bar:\n        epoch_bar.set_description(f\"Epoch: {i}\")\n        model.train()\n        for batch in train_dataloader:\n            x, y = batch\n            y = torch.unsqueeze(y, 1)\n\n            if config.use_GPU:\n                x = x.to(device)\n                y = y.to(device)\n\n            y_hat = model(x)\n            loss = loss_fn(y_hat, y)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            epoch_bar.set_postfix({\"loss\": str(loss.item())})\n        logger.log(\"loss\", loss.item())\n\n        model.eval()\n        predictions = []\n        labels = []\n        with torch.no_grad():\n            for batch in validate_dataloader:\n                x, y = batch\n                y = torch.unsqueeze(y, 1)\n\n                if config.use_GPU:\n                    x = x.to(device)\n\n                y_hat = model(x)\n\n                labels.append(y)\n                predictions.append(y_hat)\n\n        predictions = torch.cat(predictions).cpu().numpy()\n        labels = torch.cat(labels).cpu().numpy()\n        AUC, AUPR, ACC, F1_score, Precision, Recall = metric(predictions, labels)\n\n        logger.log(\"AUC\", AUC)\n        logger.log(\"AUPR\", AUPR)\n        logger.log(\"ACC\", ACC)\n        logger.log(\"F1_score\", F1_score)\n        logger.log(\"Precision\", Precision)\n        logger.log(\"Recall\", Recall)\n\n        if use_check_point:\n            max_f1 = max(logger.logs[\"F1_score\"])\n            if F1_score >= max_f1:\n                best_state_dict = model.state_dict()\n\n    if use_check_point:\n        torch.save(best_state_dict, \"output_files/check_point/best_model.ckpt\")\n\n    epoch_bar.close()\n\n\ndef test(model, test_dataloader, metric, config, logger, use_check_point=False):\n    print(\"Begin test........\")\n    device = torch.device(config.device)\n    model.to(device)\n\n    if use_check_point:\n        best_state_dict = torch.load(\"output_files/check_point/best_model.ckpt\")\n        model.load_state_dict(best_state_dict)\n\n    model.eval()\n    predictions = []\n    labels = []\n    with torch.no_grad():\n        for batch in test_dataloader:\n            x, y = batch\n\n            if config.use_GPU:\n                x = x.to(device)\n\n            y_hat = model(x)\n\n            labels.append(y)\n            predictions.append(y_hat)\n\n    predictions = torch.cat(predictions).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    AUC, AUPR, ACC, F1_score, Precision, Recall = metric(predictions, labels)\n\n    test_result = \"AUC: {:.2f} \\nAUPR: {:.2f} \\nACC: {:.2f} \\nF1_score: {:.2f} \\nPrecision: {:.2f} \\nRecall: {:.2f}\"\n    test_result = test_result.format(AUC, AUPR, ACC, F1_score, Precision, Recall)\n\n    logger.log(\"test_result\", test_result)\n\n    print(\"{:-^40s}\".format(\"Test_result\"))\n    print(test_result)\n    print(\"End test........\")\n\n\ndef main():\n    config = Config()\n    logger = DeepLog(save_path=\"output_files\")\n\n    # training config\n    config.epoch = 100\n    config.lr = 0.001\n    config.batch_size = 400\n    config.use_GPU = True\n    config.device = \"cuda:0\"\n\n    # model config\n    config.in_dim = 64\n\n    config.print_parameters()\n\n    # load dataset\n    train_dataset, validate_dataset, test_dataset = load_dataset()\n    print(\"{:-^40s}\".format(\"Dataset info\"))\n    print(\"Train_dataset: \", len(train_dataset))\n    print(\"Validate_dataset: \", len(validate_dataset))\n    print(\"Test_dataset: \", len(test_dataset))\n    print(\"All dataset: \", len(train_dataset) + len(validate_dataset) + len(test_dataset))\n    print(\"-\" * 40)\n    train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n    validate_dataloader = DataLoader(validate_dataset, batch_size=config.batch_size, shuffle=True)\n    test_dataloader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=True)\n\n    # Initialize your Model, Loss, Optimizer and Learning rate scheduler\n    model = MLP(config)\n    loss_fn = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n    # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epoch)\n\n    train(model=model,\n          optimizer=optimizer,\n          loss_fn=loss_fn,\n          train_dataloader=train_dataloader,\n          validate_dataloader=validate_dataloader,\n          test_dataloader=test_dataloader,\n          metric=get_metric,\n          config=config,\n          logger=logger,\n          use_check_point=True)\n\n    test(model=model,\n         test_dataloader=test_dataloader,\n         metric=get_metric,\n         config=config,\n         logger=logger,\n         use_check_point=True)\n\n    logger.visualization(item=\"loss\")\n    logger.visualization(item=\"ACC\")\n    logger.save(config=config)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "utils": "from sklearn import metrics\nimport numpy as np\n\n\ndef get_metric(predictions, labels):\n    \"\"\"\n    :param predictions: np.array with shape (n,)\n    :param labels: np.array with shape (n,)\n    :return: all float\n    \"\"\"\n    fpr, tpr, thresholds = metrics.roc_curve(labels, predictions)\n    Precision, Recall, thresholds = metrics.precision_recall_curve(labels, predictions)\n    AUC = metrics.auc(fpr, tpr)\n    AUPR = metrics.auc(Recall, Precision)\n\n    F1_score = 2 * (Precision * Recall) / ((Precision + Recall) + 1e-15)\n    index = np.argmax(F1_score)\n    F1_score = F1_score[index]\n    Precision = Precision[index]\n    Recall = Recall[index]\n\n    threshold = thresholds[index]\n\n    predictions[predictions > threshold] = 1\n    predictions[predictions <= threshold] = 0\n\n    ACC = metrics.accuracy_score(labels, predictions)\n\n    return AUC, AUPR, ACC, F1_score, Precision, Recall\n",
    "model": "import torch\nimport torch.nn as nn\n\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super(MLP, self).__init__()\n\n        in_dim = config.in_dim\n        self.mlp = nn.Sequential(\n            nn.Linear(in_dim, in_dim // 2),\n            nn.ReLU(),\n            nn.Linear(in_dim // 2, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.mlp(x)\n",
    "data": "import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_classification\n\n\nclass MyDataset(Dataset):\n    def __init__(self, data, label):\n        super(MyDataset, self).__init__()\n\n        self.data = data\n        self.label = label\n\n    def __getitem__(self, item):\n        data = self.data[item]\n        label = self.label[item]\n\n        return data, label\n\n    def __len__(self):\n        return self.data.shape[0]\n\n\ndef load_dataset():\n    # load your dataset here\n    data, label = make_classification(n_samples=6000, n_features=64, n_informative=32, n_clusters_per_class=1)\n    data = torch.from_numpy(data).float()\n    label = torch.from_numpy(label).float()\n    data_size = data.shape[0]\n\n    spilt_index = data_split(data_size, [0.8, 0.1, 0.1])\n    train_index, validate_index, test_index = spilt_index\n\n    train_dataset = MyDataset(data[train_index], label[train_index])\n    validate_dataset = MyDataset(data[validate_index], label[validate_index])\n    test_dataset = MyDataset(data[test_index], label[test_index])\n\n    return train_dataset, validate_dataset, test_dataset\n\n\ndef data_split(data_size, ratio, itype=\"array\"):\n    ratio = np.array(ratio)\n    numbers = (ratio * data_size).astype(np.int32)\n    cu_numbers = np.zeros(numbers.size + 1, dtype=np.int32)\n    cu_numbers[1:] = np.cumsum(numbers)\n\n    index = np.arange(data_size)\n    np.random.shuffle(index)\n    if itype == \"tensor\":\n        index = torch.from_numpy(index)\n\n    split_index = []\n    for i in range(numbers.size):\n        if i == (numbers.size - 1):\n            split_index.append(index[cu_numbers[i]:])\n        else:\n            split_index.append(index[cu_numbers[i]:cu_numbers[i + 1]])\n\n    return split_index\n"
}