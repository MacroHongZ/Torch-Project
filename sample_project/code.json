{
    "main": "# create time: 2023-11-17 11:23\n# author:  WangHZ\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom deeplog import DeepLog, Config\nfrom torch import optim\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom typing import Tuple, Dict, List\n\nfrom model import MLP\nfrom data import load_dataset\nfrom utils import classification_metric\n\n\nclass Trainer:\n    def __init__(\n        self,\n        model: nn.Module,\n        dataset: List[Dataset],\n        lr: float,\n        epoch: int,\n        batch_size: int,\n        use_GPU: bool,\n        device: str,\n        save_checkpoint: bool,\n        checkpoint_name: str,\n        visualization: bool,\n    ):\n        self.logger = DeepLog()\n\n        self.train_data = dataset[0]\n        self.validate_data = dataset[1]\n        self.test_data = dataset[2]\n\n        self.model = model\n        self.scheduler = None\n        self.loss_fn = nn.BCELoss()\n        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n        self.metric_fn = classification_metric\n\n        self.max_metric = -float(\"inf\")\n        self.lr = lr\n        self.epoch = epoch\n        self.batch_size = batch_size\n        self.use_GPU = use_GPU\n        self.device = device\n        self.save_checkpoint = save_checkpoint\n        self.checkpoint_name = checkpoint_name\n        self.visualization = visualization\n\n        if use_GPU:\n            self.model.to(self.device)\n\n    def train(self):\n        print(\"Begin training........\")\n        total_parameters = sum([param.numel() for param in self.model.parameters()])\n        print(f\"Number of parameter: {total_parameters:,d}\")\n\n        epoch_bar = tqdm(range(self.epoch))\n        for i in epoch_bar:\n            epoch_bar.set_description(f\"Epoch: {i}\")\n\n            self.model.train()\n            loss = None\n            train_dataloader = DataLoader(\n                self.train_data, batch_size=self.batch_size, shuffle=True\n            )\n            for batch in train_dataloader:\n                x, y = batch\n                y = torch.unsqueeze(y, 1)\n\n                if self.use_GPU:\n                    x = x.to(self.device)\n                    y = y.to(self.device)\n\n                self.optimizer.zero_grad()\n                y_hat = self.model(x)\n                loss = self.loss_fn(y_hat, y)\n                loss.backward()\n                self.optimizer.step()\n\n            loss_value = loss.item()\n            print(f\"Train loss: {loss_value:.4f}\")\n            self.logger.log(\"train_loss\", loss_value)\n\n            if i % 3 == 0:\n                self.validate()\n        epoch_bar.close()\n        print(\"End training........\")\n\n        if self.visualization:\n            self.logger.visualization(\"train_loss\")\n            self.logger.visualization(\"validate_loss\")\n\n    def validate(self):\n        metrics, val_loss = self.inference(self.validate_data)\n        print(f\"Validate loss: {val_loss:.4f}\")\n        self.logger.log(\"validate_loss\", val_loss)\n\n        if self.save_checkpoint:\n            auc = metrics[\"AUC_ROC\"]\n            if auc > self.max_metric:\n                self.max_metric = auc\n                self.save_model()\n\n        for key, value in metrics.items():\n            self.logger.log(key, value)\n\n    def test(self):\n        print(\"Begin test........\")\n        if self.save_checkpoint:\n            state_dict = torch.load(\"output_files/check_point/\" + self.checkpoint_name)\n            self.model.load_state_dict(state_dict)\n\n        metrics, test_loss = self.inference(self.test_data)\n\n        print(f\"{'Test result':-^40s}\")\n        for key, value in metrics.items():\n            print(f\"{key:<10}: {value:.4f}\")\n        print(\"-\" * 40)\n\n        self.logger.log(\"test_result\", metrics)\n\n    def inference(self, data: Dataset) -> Tuple[Dict[str, float], float]:\n        self.model.eval()\n        loss_values = []\n        predictions = []\n        labels = []\n        dataloader = DataLoader(data, batch_size=self.batch_size, shuffle=False)\n        with torch.no_grad():\n            for batch in dataloader:\n                x, y = batch\n                y = torch.unsqueeze(y, 1)\n\n                if self.use_GPU:\n                    x = x.to(self.device)\n                    y = y.to(self.device)\n\n                y_hat = self.model(x)\n                loss = self.loss_fn(y_hat, y)\n                loss_values.append(loss.detach().item())\n\n                labels.append(y)\n                predictions.append(y_hat)\n\n        predictions = torch.cat(predictions).cpu().numpy()\n        labels = torch.cat(labels).cpu().numpy()\n        metrics = self.metric_fn(predictions, labels)\n        mean_loss = torch.tensor(loss_values).mean().item()\n\n        return metrics, mean_loss\n\n    def save_model(self):\n        torch.save(\n            self.model.state_dict(), \"output_files/check_point/\" + self.checkpoint_name\n        )\n\n    def save_log(self, config):\n        self.logger.save(config)\n\n\ndef main():\n    config = Config()\n    config.in_dim = 64\n\n    config.lr = 0.001\n    config.epoch = 50\n    config.batch_size = 256\n\n    config.use_GPU = True\n    config.device = \"cuda:0\"\n    config.save_checkpoint = True\n    config.checkpoint_name = \"best_model.pt\"\n    config.visualization = True\n\n    dataset = load_dataset()\n    model = MLP(in_dim=config.in_dim)\n\n    trainer = Trainer(\n        model=model,\n        dataset=dataset,\n        lr=config.lr,\n        epoch=config.epoch,\n        batch_size=config.batch_size,\n        use_GPU=config.use_GPU,\n        device=config.device,\n        save_checkpoint=config.save_checkpoint,\n        checkpoint_name=config.checkpoint_name,\n        visualization=config.visualization,\n    )\n    trainer.train()\n    trainer.test()\n    trainer.save_log(config)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "utils": "from sklearn.metrics import (\n    auc,\n    accuracy_score,\n    precision_recall_curve,\n    roc_curve,\n)\n\nimport numpy as np\nfrom typing import Dict, List\n\n\ndef classification_metric(\n    predictions: np.ndarray, labels: np.ndarray\n) -> Dict[str, float]:\n    \"\"\"\n    Calculate various classification metrics.\n\n    Parameters:\n        predictions (np.ndarray): Predicted values from a classification model.\n        labels (np.ndarray): Actual labels from the data.\n\n    Returns:\n        metrics_dict (Dict[str, float]): Dictionary containing key metrics including AUC_ROC, AUC_PR, Accuracy,\n         F1_Score, Precision, Recall.\n    \"\"\"\n    fpr, tpr, roc_thresholds = roc_curve(labels, predictions)\n    precision, recall, pr_thresholds = precision_recall_curve(labels, predictions)\n    auc_roc = auc(fpr, tpr)\n    auc_pr = auc(recall, precision)\n\n    f1_scores = 2 * (precision * recall) / ((precision + recall) + 1e-15)\n    index = np.argmax(f1_scores)\n    f1_score = f1_scores[index]\n    precision = precision[index]\n    recall = recall[index]\n\n    threshold = pr_thresholds[index]\n    predictions[predictions > threshold] = 1\n    predictions[predictions <= threshold] = 0\n    accuracy = accuracy_score(labels, predictions)\n\n    metric_names = [\"AUC_ROC\", \"AUC_PR\", \"Accuracy\", \"F1_Score\", \"Precision\", \"Recall\"]\n    metric_values = [auc_roc, auc_pr, accuracy, f1_score, precision, recall]\n    metrics_dict = dict(zip(metric_names, metric_values))\n\n    return metrics_dict\n\n\ndef split_data(data_size: int, ratios: List[float], seed: int = 42) -> List[np.ndarray]:\n    \"\"\"\n    Split a dataset into multiple subsets based on given ratios.\n\n    Parameters:\n        data_size (int): Total size of the dataset.\n        ratios (List[float]): List of ratios for each subset, e.g. [0.7, 0.3, 0.05].\n        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n\n    Returns:\n        List[np.ndarray]: A list of indices for each subset.\n    \"\"\"\n    np.random.seed(seed)\n    num_subsets = len(ratios)\n    ratios_array = np.array(ratios)\n    cumulative_sizes = np.zeros(num_subsets + 1, dtype=np.int32)\n    cumulative_sizes[1:] = np.cumsum(ratios_array * data_size)\n\n    indices = np.random.permutation(data_size)\n    split_indices = [\n        indices[cumulative_sizes[i] : cumulative_sizes[i + 1]]\n        for i in range(num_subsets)\n    ]\n\n    return split_indices\n",
    "model": "import torch.nn as nn\n\n\nclass MLP(nn.Module):\n    def __init__(self, in_dim: int):\n        super().__init__()\n\n        self.mlp = nn.Sequential(\n            nn.Linear(in_dim, in_dim // 2),\n            nn.ReLU(),\n            nn.Linear(in_dim // 2, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        return self.mlp(x)\n",
    "data": "import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom typing import List\n\n\nfrom utils import split_data\n\n\nclass MyDataset(Dataset):\n    def __init__(self, data, label):\n        super(MyDataset, self).__init__()\n\n        self.data = data\n        self.label = label\n\n    def __getitem__(self, item):\n        data = self.data[item]\n        label = self.label[item]\n\n        return data, label\n\n    def __len__(self):\n        return self.data.shape[0]\n\n\ndef load_dataset() -> List[Dataset]:\n    print(\"Loading dataset...\")\n    data, labels = make_classification(\n        n_samples=6000, n_features=64, n_informative=32, n_clusters_per_class=1\n    )\n    data = torch.from_numpy(data).float()\n    labels = torch.from_numpy(labels).float()\n\n    data_size = data.shape[0]\n    split_indices = split_data(data_size, [0.8, 0.1, 0.1])\n    split_indices = [torch.from_numpy(index).to(torch.long) for index in split_indices]\n\n    datasets = [MyDataset(data[index], labels[index]) for index in split_indices]\n\n    print(\"{:-^40s}\".format(\"Dataset info\"))\n    print(\"Train_dataset: \", len(datasets[0]))\n    print(\"Validate_dataset: \", len(datasets[1]))\n    print(\"Test_dataset: \", len(datasets[2]))\n    print(\"All dataset: \", sum(len(dataset) for dataset in datasets))\n    print(\"-\" * 40)\n\n    return datasets\n"
}